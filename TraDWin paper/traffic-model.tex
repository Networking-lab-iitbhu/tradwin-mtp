%!TEX root=./paper.tex
\subsection{\name: Traffic Model}

This section outlines the architecture of our \textit{Name} model, drawing inspiration from the Generative Adversarial Imputation Nets (GAIN) \cite{gain} and Wasserstein Generative Adversarial Network (WGAN) \cite{wgan}. GAIN, a novel conditional GAN variant, has demonstrated effectiveness in various tasks involving imputation of missing values across diverse datasets. On the other hand, WGAN represents an advancement over traditional GANs, emphasizing training stability through the use of Wasserstein distance rather than Jensen-Shannon divergence. Architectural enhancements such as gradient clipping and elimination of the sigmoid function contribute to improved convergence and generation of higher-quality samples.

\subsubsection{\textit{Generator:}}

The generator \( G \) of our proposed model accepts inputs consisting of observed data with missing values, denoted as \( \mathbf{X} \), a binary mask vector \( \mathbf{M} \) that identifies the positions of missing values, and a noise vector \( \mathbf{Z} \). It generates imputed data \( \mathbf{X}_g \), which initially represents a vector of imputations, subsequently projected to form the complete imputed output \( \mathbf{Y}_0 \).

Formally, the output of the generator \( G \) can be given as: 
\[ \mathbf{X}_g = G(\mathbf{X}, \mathbf{M}, (\mathbf{1}-\mathbf{M}) \odot \mathbf{Z}) \]

Here, \( \odot \) denotes the Hadamard product, or element-wise multiplication, and \( \mathbf{Z} \) represents the \( d \)-dimensional noise vector. The generator \( G \) produces \( \mathbf{X}_g \), which fills in the missing values indicated by \( \mathbf{M} \) within the observed data \( \mathbf{X} \). The resultant output is then evaluated by the discriminator to assess its quality.


\subsubsection{\textit{Discriminator:}}

Similar to the traditional GAN framework, our model incorporates a Discriminator \( D \) alongside the Generator \( G \). The Discriminator \( D \) is designed to evaluate the imputations generated by \( G \). In the context of our adaptation inspired by GAIN, \( D \) maps the input \( \mathbf{\chi} \) to a binary vector in \( [0,1]^d \), where each component signifies whether the corresponding element in the generator's output is real (observed) or fake (imputed).

In contrast to traditional Generative Adversarial Networks (GANs), where the discriminator distinguishes between entirely real and fake outputs, the discriminator in Generative Adversarial Imputation Nets (GAIN) serves to classify individual components as real or imputed. This nuanced approach allows for a more granular evaluation of the generator's imputation performance. The loss function employed is a stochastic gradient descent (SGD) that averages the losses across these individual components, aiming to enhance the fidelity of imputed data.


\subsubsection{\textit{Hint}}
GAIN introduces an innovative concept known as the hint mechanism, characterized by a random variable \( \mathbf{H} \) that ranges over a space \( \mathcal{H} \). The primary objective of this mechanism is to furnish supplementary missing information to the discriminator concerning the binary mask. This concept was formally introduced and analyzed in \cite{gain}, where the following proposition was articulated and proved:
\begin{mdframed}[linewidth=0.5pt]
\textit{
\newtheorem{proposition}{Proposition}
\begin{proposition} \label{prop:nonunique}
    There exist distributions of \( \mathbf{X} \), \( \mathbf{M} \), and \( \mathbf{H} \) for which solutions to \( \hat{p}(\mathbf{x} | \mathbf{h}, m_i = t) = \hat{p}(\mathbf{x} | \mathbf{h}) \) for each \( i \in \{1, \ldots, d\} \), \( \mathbf{x} \in \mathcal{X} \), and \( \mathbf{h} \in \mathcal{H} \) such that \( p_h(\mathbf{h} | m_i = t) > 0 \) are not unique under the optimality criterion of GAN.
\end{proposition}
}
\end{mdframed}

This implies the existence of multiple potential distributions that the generator \( G \) might produce, appearing plausible to the discriminator \( D \). Hence, the random variable \( \mathbf{H} \) assumes the crucial role of furnishing adequate information to distinctly identify the accurate representation of the underlying data.

The hint mechanism relies on the binary mask vector \( \mathbf{M} \). For each imputed sample \( (\hat{\mathbf{x}}, \mathbf{m}) \), a random variable \( \mathbf{h} \) is drawn from the conditional distribution \( \mathbf{H} | \mathbf{M} = \mathbf{m} \). \( \mathbf{h} \) is then included as an additional input to the discriminator, altering its function to \( D: \mathcal{X} \times \mathcal{H} \rightarrow [0, 1]^d \). Each component of \( D(\hat{\mathbf{x}}, \mathbf{h}) \) now indicates the probability that the corresponding component of \( \hat{\mathbf{x}} \) was observed given \( \hat{\mathbf{X}} = \hat{\mathbf{x}} \) and \( \mathbf{H} = \mathbf{h} \). The definition of \( \mathbf{H} \) governs the level of information it provides about \( \mathbf{M} \).



% The hint mechanism depends on the binary mask vector \( \mathbf{M} \), and for each imputed sample \( (\hat{\mathbf{x}}, \mathbf{m}) \), we draw \( \mathbf{h} \) from the distribution \( \mathbf{H} | \mathbf{M} = \mathbf{m} \). We then add \( \mathbf{h} \) as an extra input to the discriminator, resulting in a function \( D: \mathcal{X} \times \mathcal{H} \rightarrow [0, 1]^d \), where each component of \( D(\hat{\mathbf{x}}, \mathbf{h}) \) represents the probability that the corresponding component of \( \hat{\mathbf{x}} \) was observed given \( \hat{\mathbf{X}} = \hat{\mathbf{x}} \) and \( \mathbf{H} = \mathbf{h} \).
% By varying how \( \mathbf{H} \) is defined, we control the level of information it provides about \( \mathbf{M} \).

\textcolor{gray!80}{\vrule width 0.95\columnwidth height 0.001pt depth 0pt \relax}

\vspace{1ex}

Traditional GANs encounter challenges such as training instability and the generation of repetitive outputs. WGAN mitigates these issues by employing the Wasserstein distance metric in place of the Jensen-Shannon divergence. This approach also incorporates architectural improvements such as gradient clipping and the elimination of the sigmoid function. Now, we discuss how these enhancements from WGAN have been integrated into GAIN.
In traditional GAN, the generator and discriminator are trained using the following loss functions:
\[
\mathcal{L}_{\text{GAN}}^G = \log(1 - D(G(z)))
\]
\[
\mathcal{L}_{\text{GAN}}^D = -\log(D(x)) - \log(1 - D(G(z)))
\]

In contrast, in WGANs, we remove the logarithms and use Wasserstein distance for loss instead.
\[
\mathcal{L}_{\text{WGAN}}^G = -D(G(z))
\]
\[
\mathcal{L}_{\text{WGAN}}^D = D(G(z)) - D(x)
\]

Another adaptation from WGANs that we integrate into GAIN is the omission of a sigmoid activation function in the discriminator's output layer, unlike in traditional GANs. This modification allows the discriminator to produce unbounded real-valued scores rather than probabilities. Additionally, WGANs utilize gradient clipping as a regularization method to improve training stability. Gradient clipping involves setting a threshold for gradients; if their magnitude exceeds this threshold, they are scaled down to prevent instability, such as exploding gradients, during training.