{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import folium\n",
    "import osmnx as ox\n",
    "from geopy import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = \"./datasets/scats/metadata.csv\"\n",
    "data_path = \"./datasets/scats/2023/july/data.csv\"\n",
    "meta_df = pd.read_csv(meta_path)\n",
    "data_df = pd.read_csv(data_path)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now I need to keep only those sites in data df which are also present in meta, since I only know locations of those sites so any other are pretty much useless to me.\n",
    "data_df only has those rows in which Site and Region is present in meta_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_site_region_pairs = meta_df[['SiteID', 'Region']].drop_duplicates()\n",
    "\n",
    "# check both Region and Site\n",
    "filtered_data_df = data_df[data_df['Site'].isin(meta_site_region_pairs['SiteID']) & data_df['Region'].isin(meta_site_region_pairs['Region'])];\n",
    "filtered_data_df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = filtered_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.drop(columns=['Avg_Volume', 'Weighted_Avg', 'Weighted_Var', 'Weighted_Std_Dev'], errors='ignore', inplace=True)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the same 'Region', 'Site' pair, sum of the 'Sum_Volume' is the total volume for that site in that region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by 'Detector'\n",
    "grouped_data_df = data_df.groupby(['End_Time', 'Region', 'Site']).agg({'Sum_Volume': 'sum'}).reset_index()\n",
    "grouped_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "# for CCITY and Site 1, is the sum 239 and End_Time 20230701000000\n",
    "data_df[(data_df['Region'] == 'CCITY') & (data_df['Site'] == 1) & (data_df['End_Time'] == 20230701000000)].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All looks good now, assign and rename some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = grouped_data_df\n",
    "data_df.rename(columns={'End_Time': 'Time', 'Sum_Volume': 'Volume'}, inplace=True)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time in format YYYYMMDDHHMMSS\n",
    "data_df['Time'] = pd.to_datetime(data_df['Time'], format='%Y%m%d%H%M%S')\n",
    "data_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add metadata to data df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.rename(columns={'SiteID': 'Site'}, inplace=True)\n",
    "\n",
    "# for all SiteID, Region pairs in data_df, add Lat, Long, Site_Type from meta_df\n",
    "data_df = data_df.merge(meta_df[['Site', 'Region', 'Lat', 'Long', 'Site_Type']], on=['Site', 'Region'], how='left')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some sites there's no meta\n",
    "data_df[data_df['Lat'].isna()]\n",
    "# count where any of the columns is na or NaN\n",
    "data_df.isna().sum()\n",
    "# drop these roww\n",
    "data_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final ready df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique site,reion pairs are there\n",
    "print(\"Total Sites:\\n\", data_df[['Site', 'Region']].drop_duplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()\n",
    "\n",
    "# ?? Why is there a 2 site difference in next timestamp filter vs no of sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data_df` is the final dataframe which has all the necessary information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting markers for time 2023-07-01 00:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_backup = data_df.copy()\n",
    "data_df = data_df[data_df['Time']==pd.to_datetime('2023-07-01 00:00:00')]\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plotting roads and the markers together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place = \"Dublin, Ireland\"\n",
    "custom_filters = '[\"highway\"~\"motorway|trunk|primary|secondary|tertiary|unclassified\"]'\n",
    "G = ox.graph_from_place(place, network_type=\"drive\", simplify=True, custom_filter=custom_filters)\n",
    "\n",
    "# simplify the network to reduce the number of nodes and edges\n",
    "G_proj = ox.project_graph(G)\n",
    "G = ox.consolidate_intersections(G_proj, tolerance=25, rebuild_graph=True, dead_ends=True)\n",
    "\n",
    "# plot the network\n",
    "gdf = ox.graph_to_gdfs(G, nodes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gdf.explore()\n",
    "\n",
    "# add markers for all sites\n",
    "for i in range(0, len(data_df)):\n",
    "    folium.Marker([data_df.iloc[i]['Lat'], data_df.iloc[i]['Long']], popup=data_df.iloc[i]['Site'], icon_size=(0.1,0.1)).add_to(m)\n",
    "\n",
    "folium.TileLayer('cartodbpositron').add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating an edge list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distances in metres between two lat, long points\n",
    "def get_dist_lat_long(lat1, long1, lat2, long2):\n",
    "    return distance.distance((lat1, long1), (lat2, long2)).m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress coordinates for data_df['Site], reindexing from Site 0 to Site 608 for a total of 609 sites\n",
    "old_site_column = data_df['Site'].copy()\n",
    "data_df['Site'] = data_df['Site'].astype('category')\n",
    "data_df['Site'] = data_df['Site'].cat.codes\n",
    "new_site_column = data_df['Site'].copy()\n",
    "\n",
    "mapping = dict(zip(old_site_column, new_site_column))\n",
    "display(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an adjancecy matrix for the sites\n",
    "site_count = len(data_df['Site'].unique())\n",
    "assert(site_count == 609 and site_count == len(data_df['Site']))\n",
    "\n",
    "adj_matrix = [[0 for i in range(site_count)] for j in range(site_count)]\n",
    "\n",
    "for i in range(0, site_count):\n",
    "    for j in range(0, site_count):\n",
    "        if i == j:\n",
    "            continue\n",
    "        adj_matrix[i][j] = get_dist_lat_long(data_df.iloc[i]['Lat'], data_df.iloc[i]['Long'], data_df.iloc[j]['Lat'], data_df.iloc[j]['Long'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a hyperparameter for the model, I will use this to generate the edge list for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off_distance = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_edges = [[0 if adj_matrix[i][j] > cut_off_distance else 1 for j in range(site_count)] for i in range(site_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of edges\n",
    "sum([sum(i) for i in binary_edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export both the adjacency matrix\n",
    "pd.DataFrame(adj_matrix).to_csv(\"./datasets/scats/2023/july/adj_matrix.csv\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the full df to same ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = data_df_backup.copy()\n",
    "full_df['Site'] = full_df['Site'].map(mapping)\n",
    "\n",
    "# save the full data\n",
    "full_df.to_csv(\"./datasets/scats/2023/july/processed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
